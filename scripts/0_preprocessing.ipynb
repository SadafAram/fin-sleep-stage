{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Import**",
   "id": "e9e4d3b45e393878"
  },
  {
   "cell_type": "code",
   "id": "1d22bc53-e028-4c22-9709-9bfda1f45fef",
   "metadata": {},
   "source": [
    "import os\n",
    "import mne\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import ntpath\n",
    "import shutil\n",
    "import dhedfreader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from mne.io import read_raw_edf\n",
    "\n",
    "mne.set_log_level('ERROR')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Preparation**\n",
    "\n",
    "In this cell, we extract the EEG signals from the PhysioNet dataset and split them by subjects."
   ],
   "id": "cefed6967f8f562f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_and_convert_edf(\n",
    "    psg_files: list[str], \n",
    "    annotation_files: list[str], \n",
    "    output_dir: str, \n",
    "    selected_channel: str, \n",
    "    wake_edge_minutes: int\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Processes EDF sleep data and converts it to NPZ format for classification.\n",
    "\n",
    "    Parameters:\n",
    "    - psg_files (list[str]): List of PSG (Polysomnography) EDF file paths.\n",
    "    - annotation_files (list[str]): Corresponding list of annotation EDF file paths.\n",
    "    - output_dir (str): Directory where processed NPZ files will be saved.\n",
    "    - selected_channel (str): EEG channel to extract data from.\n",
    "    - wake_edge_minutes (int): Number of minutes of wake data to include at the edges.\n",
    "\n",
    "    Returns:\n",
    "    - None: Saves the processed EEG and labels as NPZ files in the output directory.\n",
    "    \"\"\"\n",
    "\n",
    "    EPOCH_SEC_SIZE = 30  # Standard epoch size in seconds\n",
    "    STAGE_LABELS = {\n",
    "        \"W\": 0, \"N1\": 1, \"N2\": 2, \"N3\": 3, \"REM\": 4, \"UNKNOWN\": 5\n",
    "    }\n",
    "    ANNOTATION_TO_LABEL = {\n",
    "        \"Sleep stage W\": 0,\n",
    "        \"Sleep stage 1\": 1,\n",
    "        \"Sleep stage 2\": 2,\n",
    "        \"Sleep stage 3\": 3,\n",
    "        \"Sleep stage 4\": 3,  # Merged with stage 3\n",
    "        \"Sleep stage R\": 4,\n",
    "        \"Sleep stage ?\": 5,\n",
    "        \"Movement time\": 5\n",
    "    }\n",
    "\n",
    "    for i in tqdm(range(len(psg_files)), desc=\"Processing files\"):\n",
    "        try:\n",
    "            # Load PSG signal\n",
    "            raw = read_raw_edf(psg_files[i], preload=True, stim_channel=None)\n",
    "            sampling_rate = raw.info['sfreq']\n",
    "            raw_data = raw.to_data_frame(scalings=100.0)[selected_channel].to_frame()\n",
    "            raw_data.set_index(np.arange(len(raw_data)), inplace=True)\n",
    "\n",
    "            # Read header information from PSG and annotation files\n",
    "            with open(psg_files[i], 'r', encoding='iso-8859-1') as f:\n",
    "                reader = dhedfreader.BaseEDFReader(f)\n",
    "                reader.read_header()\n",
    "                psg_header = reader.header\n",
    "\n",
    "            with open(annotation_files[i], 'r', encoding='iso-8859-1') as f:\n",
    "                reader = dhedfreader.BaseEDFReader(f)\n",
    "                reader.read_header()\n",
    "                annotation_header = reader.header\n",
    "                _, _, annotations = list(zip(*reader.records()))\n",
    "\n",
    "            # Convert timestamps to datetime objects\n",
    "            psg_start_time = datetime.strptime(psg_header['date_time'], \"%Y-%m-%d %H:%M:%S\")\n",
    "            annotation_start_time = datetime.strptime(annotation_header['date_time'], \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            if psg_start_time != annotation_start_time:\n",
    "                raise ValueError(\"Mismatch in PSG and annotation start times.\")\n",
    "\n",
    "            # Process annotations\n",
    "            labels, valid_indices, remove_indices = [], [], []\n",
    "            for entry in annotations[0]:\n",
    "                onset_sec, duration_sec, annotation_chars = entry\n",
    "                annotation_text = \"\".join(annotation_chars)\n",
    "\n",
    "                if annotation_text not in ANNOTATION_TO_LABEL:\n",
    "                    continue\n",
    "\n",
    "                label = ANNOTATION_TO_LABEL[annotation_text]\n",
    "\n",
    "                if label != STAGE_LABELS[\"UNKNOWN\"]:\n",
    "                    if duration_sec % EPOCH_SEC_SIZE != 0:\n",
    "                        raise ValueError(\"Non-multiples of 30-second epoch found in annotation duration.\")\n",
    "\n",
    "                    num_epochs = int(duration_sec / EPOCH_SEC_SIZE)\n",
    "                    labels.append(np.full(num_epochs, label, dtype=int))\n",
    "                    index_range = int(onset_sec * sampling_rate) + np.arange(duration_sec * sampling_rate, dtype=int)\n",
    "                    valid_indices.append(index_range)\n",
    "                else:\n",
    "                    index_range = int(onset_sec * sampling_rate) + np.arange(duration_sec * sampling_rate, dtype=int)\n",
    "                    remove_indices.append(index_range)\n",
    "\n",
    "            # Flatten labels and indices\n",
    "            labels = np.hstack(labels)\n",
    "            valid_indices = np.hstack(valid_indices)\n",
    "\n",
    "            # Remove invalid indices\n",
    "            if remove_indices:\n",
    "                remove_indices = np.hstack(remove_indices)\n",
    "                selected_indices = np.setdiff1d(np.arange(len(raw_data)), remove_indices)\n",
    "            else:\n",
    "                selected_indices = np.arange(len(raw_data))\n",
    "\n",
    "            # Ensure valid indices align with available data\n",
    "            selected_indices = np.intersect1d(selected_indices, valid_indices)\n",
    "\n",
    "            # Trim excess labels if needed\n",
    "            if len(valid_indices) > len(selected_indices):\n",
    "                extra_indices = np.setdiff1d(valid_indices, selected_indices)\n",
    "                if np.all(extra_indices > selected_indices[-1]):  # Ensure trimming occurs at the end\n",
    "                    trim_count = len(selected_indices) % int(EPOCH_SEC_SIZE * sampling_rate)\n",
    "                    trim_label_count = int(math.ceil(trim_count / (EPOCH_SEC_SIZE * sampling_rate)))\n",
    "                    selected_indices = selected_indices[:-trim_count]\n",
    "                    labels = labels[:-trim_label_count]\n",
    "\n",
    "            # Extract raw EEG data\n",
    "            processed_data = raw_data.values[selected_indices]\n",
    "\n",
    "            if len(processed_data) % (EPOCH_SEC_SIZE * sampling_rate) != 0:\n",
    "                raise ValueError(\"Mismatch in EEG data length and epoch size.\")\n",
    "\n",
    "            num_epochs = len(processed_data) // (EPOCH_SEC_SIZE * sampling_rate)\n",
    "\n",
    "            # Reshape EEG data into epochs\n",
    "            if num_epochs > 0:\n",
    "                x_data = np.asarray(np.split(processed_data, num_epochs)).astype(np.float32)[:, :, 0]\n",
    "                y_labels = labels.astype(np.int32)\n",
    "\n",
    "                assert len(x_data) == len(y_labels)\n",
    "\n",
    "                # Extract wake edges\n",
    "                non_wake_indices = np.where(y_labels != STAGE_LABELS[\"W\"])[0]\n",
    "                start_idx = max(non_wake_indices[0] - (wake_edge_minutes * 2), 0)\n",
    "                end_idx = min(non_wake_indices[-1] + (wake_edge_minutes * 2), len(y_labels) - 1)\n",
    "                final_indices = np.arange(start_idx, end_idx + 1)\n",
    "\n",
    "                x_data = x_data[final_indices]\n",
    "                y_labels = y_labels[final_indices]\n",
    "\n",
    "                # Save processed data\n",
    "                output_filename = ntpath.basename(psg_files[i]).replace(\"-PSG.edf\", \".npz\")\n",
    "                save_dict = {\n",
    "                    \"x\": x_data,\n",
    "                    \"y\": y_labels,\n",
    "                    \"fs\": sampling_rate,\n",
    "                    \"ch_label\": selected_channel,\n",
    "                    \"header_raw\": psg_header,\n",
    "                    \"header_annotation\": annotation_header\n",
    "                }\n",
    "                np.savez(os.path.join(output_dir, output_filename), **save_dict)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {psg_files[i]}: {e}\")\n",
    "\n",
    "\n",
    "def convert_edf_to_npz(\n",
    "    data_path: str, \n",
    "    output_path: str, \n",
    "    channel: str = 'EEG Fpz-Cz', \n",
    "    split_ratios: tuple[float, float, float] = (0.8, 0.1, 0.1), \n",
    "    w_edge: int = 30\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Converts EDF files into NPZ format and splits them into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path (str): Path to the directory containing raw EDF files.\n",
    "    - output_path (str): Path where the processed NPZ files will be stored.\n",
    "    - channel (str): EEG channel to extract data from. Default is 'EEG Fpz-Cz'.\n",
    "    - split_ratios (tuple[float, float, float]): Ratios for train, test, and validation splits.\n",
    "    - w_edge (int): Edge window parameter for processing wake stages.\n",
    "\n",
    "    Returns:\n",
    "    - None: Saves processed files in the specified output directory.\n",
    "    \"\"\"\n",
    "\n",
    "    subdirs = ['train', 'test', 'validation']\n",
    "\n",
    "    # Validate split ratios\n",
    "    if not np.isclose(sum(split_ratios), 1.0):\n",
    "        raise ValueError(\"Split ratios must sum to 1.0 (e.g., (0.8, 0.1, 0.1)).\")\n",
    "\n",
    "    # Recreate the output directory\n",
    "    if os.path.exists(output_path):\n",
    "        shutil.rmtree(output_path)\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "    # Create subdirectories for splits\n",
    "    for subdir in subdirs:\n",
    "        os.makedirs(os.path.join(output_path, subdir))\n",
    "\n",
    "    # Fetch EDF files\n",
    "    psg_files = sorted(glob.glob(os.path.join(data_path, \"*PSG.edf\")))\n",
    "    annotation_files = sorted(glob.glob(os.path.join(data_path, \"*Hypnogram.edf\")))\n",
    "\n",
    "    if not psg_files or not annotation_files:\n",
    "        raise FileNotFoundError(\"No matching PSG or Hypnogram EDF files found in the specified directory.\")\n",
    "\n",
    "    if len(psg_files) != len(annotation_files):\n",
    "        raise ValueError(\"Mismatch in the number of PSG and Hypnogram files.\")\n",
    "\n",
    "    # Convert lists to numpy arrays for easy indexing\n",
    "    psg_files = np.array(psg_files)\n",
    "    annotation_files = np.array(annotation_files)\n",
    "\n",
    "    total_files = len(psg_files)\n",
    "    train_ratio, test_ratio, val_ratio = split_ratios\n",
    "\n",
    "    # Compute indices for splitting\n",
    "    train_end = int(train_ratio * total_files)\n",
    "    test_end = train_end + int(test_ratio * total_files)\n",
    "\n",
    "    # Split data\n",
    "    train_psg, train_ann = psg_files[:train_end], annotation_files[:train_end]\n",
    "    test_psg, test_ann = psg_files[train_end:test_end], annotation_files[train_end:test_end]\n",
    "    val_psg, val_ann = psg_files[test_end:], annotation_files[test_end:]\n",
    "\n",
    "    # Process each split\n",
    "    for subset, psg, ann in zip(subdirs, [train_psg, test_psg, val_psg], [train_ann, test_ann, val_ann]):\n",
    "        print(f\"Processing {subset} dataset ...\")\n",
    "        process_and_convert_edf(psg, ann, os.path.join(output_path, subset), channel, w_edge)\n",
    "\n",
    "\n",
    "convert_edf_to_npz(\n",
    "    data_path=\"../data/original/sleep-cassette\",\n",
    "    output_path=\"../data/split\",\n",
    "    channel=\"EEG Fpz-Cz\",\n",
    "    split_ratios=(0.8, 0.1, 0.1),\n",
    "    w_edge=30\n",
    ")"
   ],
   "id": "b15b3399c37466a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Windowing**\n",
    "\n",
    "In this cell, we validate the previous cell and extract 30-second signals per subject"
   ],
   "id": "c076b6f81d0d59a3"
  },
  {
   "cell_type": "code",
   "id": "7b5407a9-f5fd-40a8-9e97-fef7d9695d22",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "def convert_to_windows(data_path: str, file_list: list[str], save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Splits EEG recordings into smaller windows and saves them as separate NPZ files.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path (str): Path to the directory containing EEG NPZ files.\n",
    "    - file_list (list[str]): List of NPZ filenames to process.\n",
    "    - save_path (str): Directory where the processed files will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None: Saves the extracted EEG windows as individual NPZ files.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for file_name in tqdm(file_list, desc=\"Processing EEG files\"):\n",
    "        try:\n",
    "            # Load EEG sample\n",
    "            sample_path = os.path.join(data_path, file_name)\n",
    "            sample = np.load(sample_path)\n",
    "\n",
    "            # Extract EEG signal and labels\n",
    "            x_signals = sample['x']\n",
    "            y_labels = sample['y']\n",
    "\n",
    "            for idx, signal in enumerate(x_signals):\n",
    "                output_file = os.path.join(save_path, f\"{str(counter).zfill(7)}.npz\")\n",
    "                np.savez(output_file, x=signal, y=y_labels[idx])\n",
    "                counter += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "\n",
    "# Define dataset paths\n",
    "train_data_path = \"../data/split/train/\"\n",
    "test_data_path = \"../data/split/test/\"\n",
    "val_data_path = \"../data/split/validation/\"\n",
    "\n",
    "# Retrieve file lists\n",
    "train_files = os.listdir(train_data_path)\n",
    "test_files = os.listdir(test_data_path)\n",
    "val_files = os.listdir(val_data_path)\n",
    "\n",
    "# Process and save EEG windows for each dataset split\n",
    "print(\"Preparing dataset for the training set...\")\n",
    "convert_to_windows(train_data_path, train_files, \"../data/3000/train/\")\n",
    "\n",
    "print(\"Preparing dataset for the test set...\")\n",
    "convert_to_windows(test_data_path, test_files, \"../data/3000/test/\")\n",
    "\n",
    "print(\"Preparing dataset for the validation set...\")\n",
    "convert_to_windows(val_data_path, val_files, \"../data/3000/validation/\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Convert to CSV**\n",
    "\n",
    "In this cell, we convert the splitted signals into csv files."
   ],
   "id": "d6049dc7c91c1845"
  },
  {
   "cell_type": "code",
   "id": "68efdbff-404f-46f8-a3f8-3fd44450c31a",
   "metadata": {},
   "source": [
    "def convert_to_csv(data_path: str, save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Converts EEG NPZ files into a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - data_path (str): Path to the directory containing EEG NPZ files.\n",
    "    - save_path (str): File path where the final CSV will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None: Saves the EEG data as a CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the data directory exists\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Data directory '{data_path}' not found.\")\n",
    "\n",
    "    # Retrieve and shuffle file list\n",
    "    file_list = os.listdir(data_path)\n",
    "    random.shuffle(file_list)\n",
    "\n",
    "    array_list = []\n",
    "\n",
    "    for file_name in tqdm(file_list, desc=\"Processing EEG files\"):\n",
    "        try:\n",
    "            # Load EEG sample\n",
    "            sample_path = os.path.join(data_path, file_name)\n",
    "            sample = np.load(sample_path)\n",
    "\n",
    "            # Concatenate EEG signal and label\n",
    "            concatenated_data = np.append(sample['x'], sample['y'])\n",
    "            array_list.append(concatenated_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    # Convert to DataFrame and save as CSV\n",
    "    df = pd.DataFrame(np.array(array_list))\n",
    "    df.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f\"CSV file saved to {save_path}\")\n",
    "\n",
    "\n",
    "# Convert and save datasets as CSV\n",
    "print(\"Preparing CSV file for the training set...\")\n",
    "convert_to_csv(\"../data/3000/train/\", \"../data/3000/train.csv\")\n",
    "\n",
    "print(\"Preparing CSV file for the test set...\")\n",
    "convert_to_csv(\"../data/3000/test/\", \"../data/3000/test.csv\")\n",
    "\n",
    "print(\"Preparing CSV file for the validation set...\")\n",
    "convert_to_csv(\"../data/3000/validation/\", \"../data/3000/validation.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
